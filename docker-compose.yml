x-spark-common: &spark-common
  image: "bitnami/spark:3.5.2"
  volumes:
    - ./transformation/spark/jobs:/opt/bitnami/spark/jobs
    - ~/.aws/:/opt/bitnami/spark/.aws:ro
    - ./transformation/spark/spark_logs:/opt/bitnami/spark/logs

  networks:
    - airflow-spark-network

x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/scripts:/opt/airflow/scripts
    - ~/.aws/:/root/.aws:ro # Mount aws credentials
    - ./transformation:/opt/airflow/jobs

  networks:
    - airflow-spark-network

services:
  # Nessie default in-memory store
  nessie:
    image: projectnessie/nessie
    # Custom name
    container_name: nessie
    ports:
      - "19120:19120"
    profiles: [nessie]
    networks:
      - airflow-spark-network

  # Spark Master
  spark-master:
    <<: *spark-common
    container_name: spark-master
    env_file: ".env"
    # Default setting is master but ok to re-declare
    profiles: [spark]
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8081:8080"
    command: /bin/bash -c "mkdir -p /opt/bitnami/spark/logs/spark-events && /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master"

  # Spark Worker
  spark-worker-1:
    <<: *spark-common
    container_name: spark-worker-1
    env_file: ".env"
    profiles: [spark]
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    command: /bin/bash -c "mkdir -p /opt/bitnami/spark/logs/spark-events && /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"

  spark-worker-2:
    <<: *spark-common
    container_name: spark-worker-2
    env_file: ".env"
    profiles: [spark]
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    command: /bin/bash -c "mkdir -p /opt/bitnami/spark/logs/spark-events && /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"

  postgres:
    image: postgres:latest
    container_name: postgres
    ports:
      - "5432:5432"
    env_file: ".env"
    profiles: [airflow]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $POSTGRES_DB -U $POSTGRES_USER"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - airflow-spark-network

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    ports:
      - "8080:8080"
    entrypoint: ./airflow/scripts/airflow-entrypoint.sh
    env_file: ".env"
    profiles: [airflow]
    depends_on:
      - postgres

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    env_file: ".env"
    profiles: [airflow]
    depends_on:
      - postgres
    command: airflow scheduler

networks:
  airflow-spark-network:
    driver: bridge
